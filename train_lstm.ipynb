{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Convert to Sliding Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100, 52)\n",
      "(1000, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "states = np.load(\"data/states.npy\")\n",
    "labels = np.load(\"data/labels.npy\")\n",
    "\n",
    "states = states.reshape((-1, 100, 52))\n",
    "labels = labels.reshape((-1, 100, 1))\n",
    "\n",
    "print(states.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array Shape: (1000, 100, 52)\n",
      "Sliding Windows Array Shape: (95, 1000, 5, 52)\n",
      "Original Labels Shape: (1000, 100, 1)\n",
      "Sliding Windows Labels Shape: (95, 1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create sliding window arrays\n",
    "\n",
    "original_array = states\n",
    "original_labels = labels\n",
    "\n",
    "# Define window size\n",
    "window_size = 5\n",
    "\n",
    "# Calculate the number of windows\n",
    "num_windows = original_array.shape[1] - window_size + 1\n",
    "\n",
    "# Create a list to store the windows\n",
    "sliding_windows = []\n",
    "sliding_labels = []\n",
    "\n",
    "# Generate sliding windows\n",
    "for i in range(num_windows-1):\n",
    "    window = original_array[:, i : i + window_size, :]\n",
    "    lab = original_labels[:, i + window_size, :]\n",
    "    sliding_windows.append(window)\n",
    "    sliding_labels.append(lab)\n",
    "\n",
    "# Convert the list of windows to a NumPy array\n",
    "sliding_windows_array = np.array(sliding_windows)\n",
    "sliding_labels = np.array(sliding_labels)\n",
    "\n",
    "print(\"Original Array Shape:\", original_array.shape)\n",
    "print(\"Sliding Windows Array Shape:\", sliding_windows_array.shape)\n",
    "\n",
    "print(\"Original Labels Shape:\", original_labels.shape)\n",
    "print(\"Sliding Windows Labels Shape:\", sliding_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95000, 5, 52)\n",
      "(95000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Collapse into a linear dataset of windows and labels\n",
    "states_windows = sliding_windows_array.reshape((-1, 5, 52))\n",
    "labels_windows = sliding_labels.reshape((-1, 1))\n",
    "\n",
    "print(states_windows.shape)\n",
    "print(labels_windows.shape)\n",
    "\n",
    "# np.save(\"data/states_windows.npy\", states_windows)\n",
    "# np.save(\"data/labels_windows.npy\", states_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset object that will contain our sliding window data\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, states, labels):\n",
    "        self.states = states\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        state = torch.tensor(self.states[index], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "        return state, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Output Activation\n",
    "        self.out_act = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        out = self.out_act(self.fc(out[:, -1, :]))\n",
    "        # out.size() --> 100, 10\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset and Dataloader\n",
    "BATCH_SIZE = 128\n",
    "lstm_dataset = LSTMDataset(states_windows, labels_windows)\n",
    "\n",
    "train_size = int(0.8 * len(lstm_dataset))\n",
    "eval_size = len(lstm_dataset) - train_size\n",
    "\n",
    "\n",
    "train_dataset, eval_dataset = random_split(lstm_dataset, [train_size, eval_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM Model\n",
    "INPUT_DIM = 52\n",
    "HIDDEN_DIM = 100\n",
    "LAYER_DIM = 2\n",
    "OUT_DIM = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "lstm_model = LSTMModel(INPUT_DIM, HIDDEN_DIM, LAYER_DIM, OUT_DIM).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Loss Function\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\t Iteration: 100\t Loss: 0.045596010982990265\n",
      "Epoch: 1/5\t Iteration: 200\t Loss: 0.031482890248298645\n",
      "Epoch: 1/5\t Iteration: 300\t Loss: 0.008816036395728588\n",
      "Epoch: 1/5\t Iteration: 400\t Loss: 0.0010661588748916984\n",
      "Epoch: 1/5\t Iteration: 500\t Loss: 0.008033250458538532\n",
      "Epoch: 2/5\t Iteration: 600\t Loss: 0.044230662286281586\n",
      "Epoch: 2/5\t Iteration: 700\t Loss: 0.0016415275167673826\n",
      "Epoch: 2/5\t Iteration: 800\t Loss: 0.0001380309258820489\n",
      "Epoch: 2/5\t Iteration: 900\t Loss: 0.04252222925424576\n",
      "Epoch: 2/5\t Iteration: 1000\t Loss: 0.00023315039288718253\n",
      "Epoch: 2/5\t Iteration: 1100\t Loss: 0.006197980605065823\n",
      "Epoch: 3/5\t Iteration: 1200\t Loss: 0.0028245483990758657\n",
      "Epoch: 3/5\t Iteration: 1300\t Loss: 0.000746329955291003\n",
      "Epoch: 3/5\t Iteration: 1400\t Loss: 0.0005931768100708723\n",
      "Epoch: 3/5\t Iteration: 1500\t Loss: 6.634011515416205e-05\n",
      "Epoch: 3/5\t Iteration: 1600\t Loss: 0.00014222874597180635\n",
      "Epoch: 3/5\t Iteration: 1700\t Loss: 0.0005361746298149228\n",
      "Epoch: 4/5\t Iteration: 1800\t Loss: 0.001859176205471158\n",
      "Epoch: 4/5\t Iteration: 1900\t Loss: 0.0027037959080189466\n",
      "Epoch: 4/5\t Iteration: 2000\t Loss: 0.00013506512914318591\n",
      "Epoch: 4/5\t Iteration: 2100\t Loss: 9.236435289494693e-05\n",
      "Epoch: 4/5\t Iteration: 2200\t Loss: 3.147021197946742e-05\n",
      "Epoch: 4/5\t Iteration: 2300\t Loss: 0.0001333653781330213\n",
      "Epoch: 5/5\t Iteration: 2400\t Loss: 3.8415186281781644e-05\n",
      "Epoch: 5/5\t Iteration: 2500\t Loss: 0.000626034801825881\n",
      "Epoch: 5/5\t Iteration: 2600\t Loss: 1.4833078239462338e-05\n",
      "Epoch: 5/5\t Iteration: 2700\t Loss: 6.978617602726445e-05\n",
      "Epoch: 5/5\t Iteration: 2800\t Loss: 0.00030144565971568227\n",
      "Epoch: 5/5\t Iteration: 2900\t Loss: 8.458341471850872e-05\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (states, labels) in enumerate(train_loader):\n",
    "        states = states.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        preds = lstm_model(states)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 100 == 0:\n",
    "            # Calculate Loss\n",
    "            print(f'Epoch: {epoch + 1}/{NUM_EPOCHS}\\t Iteration: {iter}\\t Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.9578947368421\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # progress = tqdm.tqdm(eval_loader, total=len(eval_loader))\n",
    "    \n",
    "    # Iterate through test dataset\n",
    "    for states, labels in eval_loader:\n",
    "        images = states.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = lstm_model(images)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        # Total number of labels\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Total correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Print Accuracy\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airtrees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
